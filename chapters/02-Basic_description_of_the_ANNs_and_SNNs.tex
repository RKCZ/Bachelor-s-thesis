\chapter{Basic description of the analogue neural networks and spiking neural networks} \label{chap:02}

\section{Artificial neural networks}
Artificial neural networks are built of idealised units which compute output values from their weighted input values and continuous nonlinear activating functions \cite{tavanaeiDeepLearningSpiking2019}. The units interact with each other by propagating its continuous output. These units connect themselves into distinct layers. If there is more than one hidden layer (hidden layer is the layer which takes input from the previous layer and outputs own results to another layer, that means it is neither input layer, nor output layer), the network is recognised as a deep network. In deep networks, each layer works with a set of features received from the output of the previous layer. This feature hierarchy enables deep networks to operate accurately on large data sets with plenty of parameters. Advances in the hardware computation power made it possible to use very deep networks for various artificial intelligence tasks. However, the (still quite) significant computation costs of these networks prevent them from applications in embedded and power-constrained environments. \par
The resulting neural network needs to be learned to become functional. Learning neural network means adjusting its connection weights and other parameters to optimise the accuracy of the results. There exist two major approaches to learning of a neural network, supervised learning and unsupervised learning.

\subsection{Supervised learning in artificial neural networks}
A prerequisite for the supervised learning method is a labelled data set. Supervised learning is useful for classification or regression tasks when we need to find a set of significant relationships or structure in the input data, that means we need to approximate a regression function. The functionality of the method is dependent on the complexity of the approximated function. Higher complexity is needed to learn the full structure of the data, but there is a threat of over-fitting the model, that means the model is too complicated and fails on general data. In the neural network, the supervised learning method takes advantage of the error backpropagation algorithm. The algorithm learns the network on the learning data set and finds the difference between the computed and desired output. Then the synaptic weights are adjusted to minimise the difference \cite{sathyaComparisonSupervisedUnsupervised2013}.

\subsection{Unsupervised learning in artificial neural networks}
Unsupervised learning means that the data set is missing labels, and the model attempts to find some structure in the underlying data set. Common use-cases for unsupervised learning are exploratory analysis and dimensional reduction. Exploratory analysis can get a fundamental inspection of the data in situations where it is difficult or impossible for people to find trends in the data. Typical applications for unsupervised learning methods are data clustering or visualisation. Another example of unsupervised learning is generative adversarial networks (GANs). GANs is an architecture that uses two neural networks to generate new synthetic data with a structure similar to real data \cite{goodfellowGenerativeAdversarialNets2014}.

\section{Spiking neural networks}
Although the biological neural networks inspired the basic principles of artificial neural networks, the processes in the biological networks differ from the processes in ANNs. The spiking neural networks proceed from the knowledge retrieved from observations of the biological neural networks. The basic idea behind SNN is that the network consists of the spiking neurons which have a different characteristic than artificial neurons. Each spiking neuron has a specified threshold value and membrane potential. Note that the resting membrane potential of the real biological neuron is typically negative. However, for mathematical simplification, it is convenient to assume in the spiking neuron models, that the resting membrane potential is zero, and thus the current membrane potential is the sum of postsynaptic potentials \cite{maassNetworksSpikingNeurons1997}. The postsynaptic potential is a result of an action potential (spike) generated by previous neurons which connects to the observed neuron. The postsynaptic potential can be either excitatory or inhibitory. The excitatory postsynaptic potential (EPSP) increments (depolarise) the membrane potential and the inhibitory postsynaptic potential decrements (hyper-polarise) the membrane potential. When the membrane potential reaches the threshold value of the neuron, a spike (a sudden increase of voltage) is generated. It can also be said that the neuron fired. Depending on the level of detail, various models of spiking neuron exist. Popular spiking neuron models are the leaky integrate-and-fire model (LIF), the Izhikevich neuron model or spike response model \cite{tavanaeiDeepLearningSpiking2019}. \par
Besides the biological perspective on spiking neural networks, there also exists a more technical point of view used in neuromorphic engineering. In this perspective, the spikes are more often called events. This term originates from the address event representation protocol \cite{pazTestInfrastructureAddressEventRepresentation2005, boahenPointtopointConnectivity00} which is used to connect event-based neuromorphic peripherals. With this point of view, the emphasis on the biological plausibility of the model and level of detail of the neurons steps back to allow more pragmatic ways to use the networks in such neuromorphic applications. The combination of the spiking neural networks with event-based sensors is especially useful because it enables both parts to utilise its full potential in terms of power efficiency \cite{pfeifferDeepLearningSpiking2018}.

\section{Deep spiking neural networks}
Similar to ANNs, if the spiking neural network consists of multiple layers, and at least one of them is hidden, the network is called deep. Deep neural networks, both spiking and non-spiking, are more capable. The interconnected neurons transfer information between themselves by firing trains of spikes (spike trains). Strictly speaking, the information is coded in the number of spikes and their frequencies, rather than amplitude characteristic of a single spike. That means the communication in the network is discrete in time in contrast to ANN, where the communication is going on continuous activation functions. This characteristic makes the spiking neural networks interesting to research because SNNs can be used to create more efficient low-energy consuming neural networks. Also, SNNs can be implemented on specialised dedicated hardware, which even more improves energy efficiency. Another advantage of the deep spiking networks is that the approximation of the final layer output is retrievable at the time of recording the first input spikes and the approximation improves over time \cite{pfeifferDeepLearningSpiking2018}.
Deep spiking neural networks have theoretically the same representational power as deep ANNs with lower energy requirements. Nevertheless, there is a problem with achieving the same results as with ANNs because optimal solutions for supervised learning of the SNNs do not exist yet. The gradient-based optimisation methods used in the artificial neural networks require the activation function to be differentiable \cite{tavanaeiDeepLearningSpiking2019}. The spike trains generated in spiking neurons can be formally represented by sums of delta functions which do not have derivatives. Multiple approaches proposed how to overcome this problem, but the research of the learning methods for the SNNs is still at its beginning \cite{tavanaeiDeepLearningSpiking2019}. 

\section{State of the art}
Recently there appeared many novel theories and experiments on the topic of the deep spiking neural networks and new approaches to learning such networks. As stated above, traditional methods for deep learning can be hardly used because of the distinct characteristics of the two models. One of the approaches to overcome this issue is using some sort of approximate derivatives or other substitutes, although this may reduce bio-plausibility of the model. \cite{leeTrainingDeepSpiking2016} proposed a spiking network backpropagation rule with low-pass filtering to handle discontinuity at the time of the spike. This method demonstrates state-of-the-art results for deep SNNs.\par
The more biologically plausible approach is using local learning rules such as spike-timing-dependent plasticity (STDP). STDP adjusts the synaptic weights between presynaptic and postsynaptic neuron according to the relative difference of their spike times. If presynaptic neuron fires relatively briefly before the postsynaptic neuron, their synaptic weight increments. The weight decrements in the opposite case, that is in the case when presynaptic neuron spikes briefly after the postsynaptic neuron. Local learning rules can be used with unsupervised learning \cite{tavanaeiDeepLearningSpiking2019}, but with supervised learning, there is once again missing a plausible backpropagation method. This problem can be covered, for example, by introducing recurrent connections to propagate the error signal. The use of local learning rules is also interesting for efficient implementations on dedicated hardware, such as SpiNNaker\cite{furberSpiNNakerProject14} or BrainScales. At present, the efficiency of the networks which use this method falls behind in terms of precision. \par
Another approach is to create a conversion between ANN and SNN. For the conversion, a conventional deep network trains using backpropagation and other methods available in ANNs. Then the already trained network is transformed into its spiking variant. The conversion is done by mapping the analogue neurons onto spiking ones (the mapping can be both one-to-one and one-to-many) and adjusting the weights and parameters of the spiking neurons according to the trained analogue network. PÃ©rez-Carrasco first proposed the conversion approach in \cite{perez-carrascoMappingFrameDriven13}. This approach can enhance the performance of hardware implementations of deep neural networks. An advantage of this approach is that the best, state-of-the-art deep networks can be transformed into SNNs without previous modifications and the efficiency of the final SNN is approaching the efficiency of the original network. However not every ANN can be converted, because features or methods which are common in analogue networks might not have its spiking equivalents. \cite{rueckauerConversionContinuousValuedDeep2017} introduced additional techniques to convert more general class of ANNs by implementing features such as soft-max, batch normalisation or max-pooling (using a maximum filter for spatial reduction of the input feature set), which were previously not available in the spiking networks. The resulting accuracy of experiments which used the conversion approach is very promising \cite{tavanaeiDeepLearningSpiking2019, pfeifferDeepLearningSpiking2018}. One of the disadvantages of the conversion is that the most often used technique uses rate codes which are quite inefficient. Rate-coding means that the activation values of the original analogue deep network translate into firing rates of the spiking neurons, so multiple spikes are necessary to stand for a single activation value. Alternative spike codes need to be developed to overcome this. \cite{zambranoFastEfficientAsynchronous2016} proposes one such coding, which maintains the accuracy of the modelled artificial network while reducing the total count of spikes needed in comparison to other conversion methods. \par
There exists a modified hybrid approach known as \textit{constrain-then-train}. Instead of starting with conventional learning of the ANN, additional constraints needed for spiking networks simulation (or neuromorphic hardware) are applied on the first network before its training. Then, the learned parameters of such constrained ANN can be used directly during the mapping onto SNN without further re-scaling \cite{pfeifferDeepLearningSpiking2018}. The retrieved parameters are particular for just single settings of the spiking neuron model, so the network needs retraining if these parameters should change. The final network has an advantage that it adapts better to the target environment than generally converted networks. \par
Above, four approaches to supervised learning of the spiking neural networks are briefly described. The spike-based learning and local learning methods have a common need for improvement of the accuracy to match the results retrieved by the other two methods. However, they offer other notable features such as potentially better energy efficiency and the ability to exploit additional features of the neuromorphic hardware (e.g. precise timing). Also, inferior performance efficiency is being improved by recent works. On the other hand, the conversion techniques are comparable to its original artificial network models, but due to the used coding, its energy efficiency deteriorates.

\subsection{Binary deep neural networks}
Binary deep neural networks (BNNs) are an alternative to spiking neural networks in terms of energy efficiency. BNNs are deep neural networks that use binary activation values and binary-valued weights \cite{simonsReviewBinarized19}. The state-of-the-art binary neural networks achieve slightly degraded accuracy in comparison with non-binary full precision networks, but with reduced memory requirements and less power consumption \cite{courbariauxBinarizedNeural16}. This advantage is more evident if both activation values and weights are binary as more complicated arithmetic operations can be replaced with simpler bitwise operations \cite{kimBitwiseNeural16}. With binary activations, BNNs can also directly use spikes from event-based hardware \cite{linAccurateBinary17}. Compared to spiking networks, the information is still propagated synchronously and does not allow the fast propagation of most salient features \cite{pfeifferDeepLearningSpiking2018}.

\subsection{Comparison}
Standard classification benchmarks are usually used to compare the individual works, although \cite{pfeifferDeepLearningSpiking2018} points out that these benchmarks might not be appropriate for evaluation of spiking networks and should be taken as a proof of concept. The most used benchmark for demonstrating the performance of the spiking network is the MNIST data set of handwritten digits \cite{lecunGradientbasedLearning98}. \Cref{tab:MNIST_benchmark} compares some of the reported classification accuracies on the MNIST data set.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\linewidth}{>{\raggedright\arraybackslash}Xcc}
    \toprule
        description & type & accuracy (\%) \\
        \midrule
        Rate-based conversion of 7-layer CNN with max-pooling \cite{rueckauerConversionContinuousValuedDeep2017} & SNN & 99.44 \\
        Converted SNN with pulsed Sigma-Delta coding scheme \cite{zambranoFastEfficientAsynchronous2016} & SNN & 99.14 \\
        Converted Lenet-5 with TTFS temporal coding \cite{rueckauerConversionAnalogSpiking2018} & SNN & 98.57 \\
        Spiking CNN with low-pass backpropagation rule \cite{leeTrainingDeepSpiking2016} & SNN & 99.31 \\
        Branching \& Merging CNN with Homogenous filter capsules \cite{kalganovaBranchingMerging20} & ANN & 99.79 \\
        Multi-column deep neural network \cite{cireganMulticolumnDeep12} & ANN & 99.77 \\
        \bottomrule
    \end{tabularx}
    \caption{Comparison of selected deep neural networks.}
    \label{tab:MNIST_benchmark}
\end{table}