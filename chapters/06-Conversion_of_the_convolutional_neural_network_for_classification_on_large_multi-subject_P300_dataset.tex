\chapter{Conversion of the convolutional neural network for classification on large multi-subject P300 dataset} \label{chap:06}

A convolutional neural network described in \cite{varekaEvaluationConvolutional20} was converted with the SNN conversion toolbox. SNN-TB was selected for the conversion over Nengo simulation platform because the CNN contained batch normalization layer and softmax activation function, which prevented Nengo to convert the network successfully at the time. \par
The original CNN was used for binary classification of event-related potential (ERP) signals retrieved from large multi-subject P300 dataset, which was presented in \cite{moucekEventrelatedPotential17}. The dataset was retrieved from EEG experiments, where subjects were told to concentrate on a single-digit number (from 0 to 9), and experimenters were trying to guess the number, which the subject selected, from recorded EEG data while the subject was stimulated with random sequences of all digits. \cite{varekaEvaluationConvolutional20} extracted short intervals of signal around all target stimuli from this dataset. An equal amount of non-target samples was acquired randomly from the remaining data. This newly-emerged dataset consisted of two distinct equal-sized classes, the target epochs (i.e. short time intervals of EEG data around the stimulus, which was the subject's thought number) and the non-target epochs. Each epoch contained three EEG channels and was 1200 ms long. The sampling frequency was 1 kHz. That means that each sample of the network input was a $3 \times 1200$ matrix. \par
The input data were preprocessed in a similar fashion as in \cite{varekaEvaluationConvolutional20}. For the purpose of this work was used the already modified subset \cite{moucekReplicationData19} of the original dataset, which was created during the classification with the original CNN and was publicly available. The input file contained one matrix of feature vectors for all target data and one matrix of feature vectors for all non-target data. Those matrices were concatenated to create the input vectors. The corresponding ground truth labels were produced by the one-hot encoding of the input data. Then, severely damaged epochs were discarded from the input the same way as in the original article \cite{varekaEvaluationConvolutional20}. It was done by discarding all epochs, where amplitudes were higher than 100 $\mu V$ in any of the three channels. These preprocessed data were divided into training and test subsets by randomly picking out 25\% of the dataset samples into the test set. The test set was saved to the filesystem so it could be passed to SNN-TB during conversion later. \par
The network architecture of the CNN model was replicated entirely from the original work. This model was cross-validated the same way as in the original article. That means 30 iterations of the Monte-Carlo cross-validation were performed, and each iteration held out 25\% of the training set to create validation subset. In each iteration, a new instance of the network was created and trained. The training was done in 30 epochs while early stopping with patience value of 5 was used. After that, the trained ANN was evaluated on the test set. 10\% of the dataset used during training were saved for normalization of the layer weights during conversion. The evaluated ANN was parsed into an internal Keras representation of the SNN-TB, weights of the parsed model were normalized with previously saved part of the training data. The normalized model was converted into a spiking model, and the spiking model was evaluated on the test set each iteration. \par
The evaluation of each input sample of the spiking network was simulated for 50 timesteps. The simulator evaluated the whole test set gradually in batches of 49 samples.