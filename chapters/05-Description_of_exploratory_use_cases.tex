\chapter{Description of exploratory use cases} \label{chap:05}

This chapter describes the initial experiments with the Nengo simulator and the SNN-Toolbox, which were selected as the most appropriate tools for subsequent use in this work. The decision was based on the pieces of information stated about the individual tools in the previous chapters. Both tools make it possible to use analogue neural networks to train the model and then convert it to the corresponding spiking neural network for inference. This approach is more utilisable for practical needs at the moment because other approaches of using SNNs suffer from issues described in \cref{chap:02} above. Thus, it is complicated to use them for any practical application such as image recognition. However, they are still necessary for research in the area of spiking networks and scientific experiments. Furthermore, the other simulators proved to be quite hard to work with, for example because of unreliable interfaces of their dependencies or other issues. \par
In the beginning, the Nengo simulator was inspected more thoroughly. It helps to new users with its simple graphical interface, which contains several tutorials and examples. The tutorials focus mainly on an explanation of the Neural Engineering Framework and Semantic Pointer Architecture methods introduced by authors of the Nengo simulator, rather than on SNNs generally. The GUI is divided into a script editor and visualisation area, where are visualised the objects created in the script editor in actual time. These visualisations help understand the whole course of the simulation of the spiking neural network. An additional package \texttt{nengo-gui} must be installed to access the graphical environment. The environment is an HTML5-based application, which is accessed through an internet browser. The interactive editor supports Nengo and NengoDL backends for running the simulation. \par
% TODO describe Spaun execution details
The next goal was to execute the Spaun 2.0 model to inspect the capabilities of the Nengo even further. The objective was to simulate the handwritten digit recognition, which is one of the cognitive tasks that Spaun 2.0 supports. Although this use case should be quite straightforward according to the documentation of the Nengo, there appeared a few issues. Firstly, there does not exist any documentation for the model. It means that the only way how to get information about its execution is to correspond with its authors about every detail, for example, to discover that the model is not compatible with recent versions of the Nengo package. The other problem is probably caused by the fact that the Spaun 2.0 model can require more than 26 gigabytes of memory to execute. Even though the model was executed on a cloud platform, which provides such resources, the program still finished the simulation with memory allocation errors.

\section{Using the same pre-trained model for the Nengo and SNN-Toolbox}
The next objective was to assess conversion abilities of both Nengo and SNN conversion toolbox on a similar use case. Conversion of a simple neural network model represented by Keras interface was selected as a suitable use case. The model was assembled and trained as an ordinary analogue neural network on an MNIST dataset. The model architecture must have been constrained to allow comparison of the two convertors because Nengo currently does not support batch normalization. Although it should fall back to ANN implementation for unsupported layers, it did not work if batch normalization layers were in the model.\par
There was an issue with using the same network model directly because there exist two versions of Keras framework at this time. SNN-TB uses a standalone multi-backend version of Keras, which supports Theano and CNTK toolkits alongside with the TensorFlow backend. However, this version of Keras will not be further developed in the future. The other version of Keras is integrated directly into TensorFlow as its high-level API for neural network development. This version of Keras interface is used in Nengo platform. However, both versions work similarly, and the model exported from the multi-backend version can be migrated into the other without further complications.

\subsection{Model architecture}
The final selected model was a convolutional network, which consisted of multiple consecutive convolutional layers, and contained an average pooling layer. A visualisation of the network architecture is shown in \cref{fig:keras_model}. The first convolution layer was consisted of thirty-two filters. The kernel size was $3 \times 3$. The second convolutional layer had sixty-four $3 \times 3$ filters with a stride length of $2 \times 2$. All layers have rectified linear unit (ReLU) as activation function, and the weight matrices were initialized with He uniform initializer. The only exception is the last dense layer, which uses softmax activation. The presented model was configured to use the Adam \cite{kingmaAdamMethod17} optimizer and categorical cross-entropy as a loss function. The trained Keras model was exported to the filesystem. \par
\begin{figure}[htbp]
    \centering
    \includesvg[inkscapelatex=false, height=0.65\textheight]{images/keras_model}
    \caption{The Keras model of a convolutional network used for classification of the MNIST dataset.}
    \label{fig:keras_model}
\end{figure}
Conversion of the network with SNN-TB is quite straightforward. There must be created a configuration file, which contains all necessary parameters that must be passed to the \texttt{main} function of the toolbox. The configuration file can be written with a few simple utility functions of the SNN-TB. The configuration file contains paths to the model and dataset as well as a specification of the used simulator and its parameters, parameters of the simulated spiking neuron cells or plots and variable logs, which should be generated by the toolbox. The SNN-TB then automatically performs the conversion of the network and evaluates it on the given test samples. There is an option to evaluate the ANN input model so the results can be compared instantly. \par
% TODO describe precise steps to convert and simulate SNN with Nengo
Some adjustments need to be done to simulate a corresponding spiking network in the Nengo simulator. The input samples must be reformatted to conform with the format which is required by the Nengo simulator. More precisely, there must be added a dimension to the data array to spread the data over time of the simulation, and the input image needs to be flattened into a single dimension. The \texttt{nengo\_dl.Converter} class converts the ANN. Then the simulation is executed the same way as other simulations in Nengo.
