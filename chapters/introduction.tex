A neural network is a set or population of specialized cells (neurons) that are interconnected by synapses. In biology, neural network forms the structure of a neural system in animals. The interconnection pattern, size and spatial organization of the population determines the architecture and specialization of the network. The network specializes to carry out a specific function when it is activated. These specialized networks then tie to one another to develop larger systems (e.g. animal brain). These natural phenomena inspired computer scientists to design a set of algorithms that models some aspects of the animal brain. 

Artificial (analogue) neural networks (ANNs) were initially inspired by biological neural systems but many concepts were simplified or modified to conform to their practical applications. On the other hand, the spiking neural networks (SNNs) were mostly meant to simulate their biological models as closely as possible and thus improve our understanding of how real biological networks achieve their cognitive abilities with incredibly low energy consumption. Although the differences between those approaches still last, some aspects of the two approaches are getting more interconnected as the research advances and one approach can exploit the other. Nowadays, deep ANNs achieve satisfying accuracy in many complex tasks, although with extensive requirements on computation power. Because of that, new goals for the improvement of their efficiency are being set. That is where deep SNNs appear to be an interesting topic of research because of better power efficiency \cite{caoSpikingDeepConvolutional2015, tavanaeiDeepLearningSpiking2019}, although the applications generally do not accomplish the same accuracy yet. Apart from the applications in which ANNs are used, spiking networks open up new fields of utilization. In the following sections, I will briefly summarize the differences between the two classes of neural networks and then focus on the current state of the research in the area of the spiking networks.

\section{Artificial neural networks}
Artificial neural networks are built of idealised units which compute output values from their weighted input values and continuous nonlinear activating functions \cite{tavanaeiDeepLearningSpiking2019}. The units interact with each other by propagating its continuous output. These units can be connected into distinct layers. If there is more than one hidden layer (hidden layer is the layer which takes input from the previous layer and outputs own results to another layer, that means it is neither input layer, nor output layer), the network is recognized as a deep network. In deep networks, each layer works with a set of features received from the output of the previous layer. This feature hierarchy enables deep networks to operate accurately on large data sets with plenty of parameters. Advances in the hardware computation power made possible to use very deep networks for various artificial intelligence tasks. However, the (still quite) great computation costs of these networks prevent them from applications in embedded and power-constrained environments. \par
To become functional, the resulting neural network needs to be learned. Learning neural network means adjusting its connection weights and other parameters to optimise the accuracy of the results. There exist two major approaches to learning of a neural network, supervised learning and unsupervised learning. \par

\subsection{Supervised learning in artificial neural networks}
A prerequisite for the supervised learning method is a labelled data set. Supervised learning is useful for classification or regression tasks when we need to find a set of significant relationships or structure in the input data, that means we need to approximate a regression function. The functionality of the method is dependent on the complexity of the function which is being learned. Higher complexity is needed to learn the full structure of the data, but there is a threat of over-fitting the model, that means the model will be too complex and will fail on general data. In the neural network, supervised learning is achieved with the error backpropagation algorithm. The algorithm learns the network on the learning data set and finds the difference between the computed and desired output. Then the synaptic weights are adjusted to minimise the difference \cite{sathyaComparisonSupervisedUnsupervised2013}.

\subsection{Unsupervised learning in artificial neural networks}
Unsupervised learning means that the data set is missing labels and the model attempts to find some structure in the underlying data set. Common use-cases for unsupervised learning are exploratory analysis and dimensional reduction. Exploratory analysis can be used to get an initial overview in situations where it is difficult or impossible for people to find trends in the data. Typical applications for unsupervised learning methods are data clustering or visualisation. Another example of unsupervised learning is generative adversarial networks (GANs). GANs is an architecture that uses two neural networks to generate new synthetic data with a structure similar to real data \cite{goodfellowGenerativeAdversarialNets2014}.

\section{Spiking neural networks}
Although the basic principle of artificial neural networks is inspired by biological neural networks, the processes in the biological networks differ from the processes in ANNs. The spiking neural networks are built upon knowledge retrieved from observations of the biological neural networks. The basic idea behind SNN is that the network consists of the spiking neurons which have a different characteristic than artificial neurons. Each spiking neuron has a specified threshold value and membrane potential. Note that the resting membrane potential of the real biological neuron is typically negative, but for mathematical simplification, it is convenient to assume in the spiking neuron models, that the resting membrane potential is zero and thus the current membrane potential is the sum of postsynaptic potentials \cite{maassNetworksSpikingNeurons1997}. The postsynaptic potential is a result of action potential (spike) generated by previous neurons which are connected to the observed neuron. The postsynaptic potential can be either excitatory or inhibitory. The excitatory postsynaptic potential (EPSP) increments (depolarise) the membrane potential and the inhibitory postsynaptic potential decrements (hyper-polarise) the membrane potential. When the membrane potential reaches the threshold value of the neuron, a spike (a sudden increase of voltage) is generated. It can also be said that the neuron fired. Depending on the level of detail, various models of spiking neuron exist. Popular spiking neuron models are the leaky integrate-and-fire model (LIF), the Izhikevich neuron model or spike response model \cite{tavanaeiDeepLearningSpiking2019}. \par
Besides the biological perspective on spiking neural networks, there also exists a more technical point of view used in neuromorphic engineering. In this perspective, the spikes are more often called events. This term originates from the address event representation protocol \cite{pazTestInfrastructureAddressEventRepresentation2005, boahenPointtopointConnectivity00} which is used to connect event-based neuromorphic peripherals. With this point of view, the emphasis on the biological plausibility of the model and level of detail of the neurons is sidelined to allow more pragmatic ways to use the networks in such neuromorphic applications. The combination of the spiking neural networks with event-based sensors is especially useful because it enables both parts to utilise its full potential in terms of power efficiency \cite{pfeifferDeepLearningSpiking2018}.

\section{Deep spiking neural networks}
Similar to ANNs, if the spiking neural network consists of multiple layers and at least one of them is hidden, the network is called deep. Deep neural networks, both spiking and non-spiking are more capable. The interconnected neurons transfer information between themselves by firing trains of spikes (spike trains). Strictly speaking, the information is coded in the number of spikes and their frequencies, rather than the amplitude characteristic of a single spike. That means the communication in the network is discrete in time in contrast to ANN where the communication is based on continuous activation functions. This makes the spiking neural networks interesting to research because SNNs can be used to create more efficient low-energy consuming neural networks. Also, SNNs can be implemented on special dedicated hardware, which even more improves energy efficiency. Another advantage of the spiking deep networks is that the approximation of the final layer output can be retrieved at the time of recording first input spikes and the approximation improves over time \cite{pfeifferDeepLearningSpiking2018}.
Deep spiking neural networks have theoretically the same representational power as deep ANNs with lower energy requirements. But there is a problem with achieving the same results as with ANNs because optimal solutions for supervised learning of the SNNs do not exist yet. The gradient-based optimisation methods used in the artificial neural networks require the activation function to be differentiable \cite{tavanaeiDeepLearningSpiking2019}. The spike trains generated in spiking neurons can be formally represented by sums of delta functions which do not have derivatives. To overcome this problem, multiple approaches were proposed, but the research of the learning methods for the SNNs is still at its beginning \cite{tavanaeiDeepLearningSpiking2019}. 

\section{State of the art}
Recently there appeared many novel theories and experiments on the topic of the deep spiking neural networks and new approaches to learning such networks. As stated above, traditional methods for deep learning can be hardly used because of the distinct characteristics of the two models. One of the approaches to overcome this issue is using some sort of approximate derivatives or other substitutes, although this may reduce bio-plausibility of the model. \cite{leeTrainingDeepSpiking2016} proposed a spiking network backpropagation rule with low-pass filtering to handle discontinuity at the time of the spike. This method demonstrates state-of-the-art results for deep SNNs.\par
The more biologically plausible approach is using local learning rules such as spike-timing-dependent plasticity (STDP). STDP adjusts the synaptic weights between presynaptic and postsynaptic neuron according to the relative difference of their spike times. If presynaptic neuron fires relatively briefly before the postsynaptic neuron, their synaptic weight is increased. The weight is decreased in the opposite case, that is in the case when presynaptic neuron spikes briefly after the postsynaptic neuron. Local learning rules can be used with unsupervised learning \cite{tavanaeiDeepLearningSpiking2019}, but with supervised learning, there is once again missing a plausible backpropagation method. This problem can be covered for example by introducing recurrent connections to propagate the error signal. The use of local learning rules is also interesting for efficient implementations on dedicated hardware, such as SpiNNaker\cite{furberSpiNNakerProject14} or BrainScales. At present, the efficiency of the networks which use this method falls behind in terms of precision. \par
Another approach is to create a conversion between ANN and SNN. For the conversion, a conventional deep network is trained using backpropagation and other methods available in ANNs. Then the already trained network is transformed into its spiking variant. This is done by mapping the analogue neurons onto spiking ones (the mapping can be both one-to-one and one-to-many) and adjusting the weights and parameters of the spiking neurons according to the trained analogue network. The conversion approach was first proposed in \cite{perez-carrascoMappingFrameDriven13}. This approach can be used to enhance the performance of hardware implementations of deep neural networks. An advantage of this approach is that the best, state-of-the-art deep networks can be transformed into SNNs without previous modifications and the efficiency of the final SNN is approaching the efficiency of the original network. However not every ANN can be converted, because features or methods which are common in analogue networks might not have its spiking equivalents. \cite{rueckauerConversionContinuousValuedDeep2017} introduced additional techniques to convert more general class of ANNs by implementing features such as soft-max, batch normalisation or max-pooling (using a maximum filter for spatial reduction of the input feature set), which were previously not available in the spiking networks. The resulting accuracy of experiments which used the conversion approach is very promising \cite{tavanaeiDeepLearningSpiking2019, pfeifferDeepLearningSpiking2018}. One of the disadvantages of the conversion is that the most often used technique uses rate codes which are quite inefficient. Rate-coding means that the activation values of the original analogue deep network are translated into firing rates of the spiking neurons, so multiple spikes are necessary to stand for a single activation value. To overcome this, alternative spike codes are needed to be developed. \cite{zambranoFastEfficientAsynchronous2016} proposes one such coding, which maintains the accuracy of the original artificial network while reducing the total count of spikes needed in comparison to other conversion methods. \par
There exists a modified hybrid approach known as \textit{constrain-then-train}. Instead of starting with conventional learning of the ANN, additional constraints needed for spiking networks (or target neuromorphic platform) are applied on the initial network before it is trained. Then, the learned parameters of such constrained ANN can be used directly during the mapping onto SNN without further re-scaling \cite{pfeifferDeepLearningSpiking2018}. The retrieved parameters are particular for just one setting of the spiking neuron model, so the network needs to be retrained if these parameters should change. The final network has an advantage that it adapts better to the target environment than generally converted networks. \par
Above four approaches to supervised learning of the spiking neural networks are briefly described. The spike-based learning and local learning methods have a common need for improvement of the accuracy to match the results retrieved by the other two methods, but they offer other notable features such as potentially better energy efficiency and the ability to exploit additional features of the neuromorphic hardware (e.g. precise timing). Also, inferior performance efficiency is being improved by recent works. On the other hand, the conversion techniques are comparable to its artificial network original models, but due to the used coding, its energy efficiency is deteriorated.

\subsection{Binary deep neural networks}
Binary deep neural networks (BNNs) are an alternative to spiking neural networks in terms of energy efficiency. BNNs are deep neural networks that use binary activation values and/or binary-valued weights \cite{simonsReviewBinarized19}. The state-of-the-art binary neural networks achieve slightly degraded accuracy in comparison with non-binary full precision networks, but with reduced memory requirements and less power consumption \cite{courbariauxBinarizedNeural16}. This is more evident if both activation values and weights are binary as more complex arithmetic operations can be replaced with simpler bitwise operations \cite{kimBitwiseNeural16}. With binary activations, BNNs can also directly use spikes from event-based hardware \cite{linAccurateBinary17}. Compared to spiking networks, the information is still propagated synchronously and does not allow the fast propagation of most salient features \cite{pfeifferDeepLearningSpiking2018}.

\subsection{Comparison}
To compare the individual works, common classification benchmarks are used usually, although \cite{pfeifferDeepLearningSpiking2018} points out that these benchmarks might not be appropriate for evaluation of spiking networks and should be taken as a proof of concept. The most used benchmark for demonstrating the performance of the spiking network is the MNIST data set of handwritten digits. Table \ref{tab:MNIST_benchmark} compares some of the reported classification accuracies on the MNIST data set.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\linewidth}{|>{\raggedright\arraybackslash}X|c|c|}
    \hline
        description & type & accuracy (\%) \\
        \hline
        \hline
        Rate-based conversion of 7-layer CNN with max-pooling \cite{rueckauerConversionContinuousValuedDeep2017} & SNN & 99.44 \\
        \hline
        Converted SNN with pulsed Sigma-Delta coding scheme \cite{zambranoFastEfficientAsynchronous2016} & SNN & 99.14 \\
        \hline
        Converted Lenet-5 with TTFS temporal coding \cite{rueckauerConversionAnalogSpiking2018} & SNN & 98.57 \\
        \hline
        Spiking CNN with low-pass backpropagation rule \cite{leeTrainingDeepSpiking2016} & SNN & 99.31 \\
        \hline
        Branching \& Merging CNN with Homogenous filter capsules \cite{kalganovaBranchingMerging20} & ANN & 99.79 \\
        \hline
        Multi-column deep neural network \cite{cireganMulticolumnDeep12} & ANN & 99.77 \\
    \hline
    \end{tabularx}
    \caption{Comparison of selected deep neural networks.}
    \label{tab:MNIST_benchmark}
\end{table}